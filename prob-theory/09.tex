\chapter{Multivariate discrete distribution and Independence}


\begin{eg}
	Roll 2 dice (blue and red). So, 
	$$
		\Omega = \{(x, y) \mid 1 \le x, y \le 6 \}
	$$
	
	\vspace{1em}
	
	We have X outcome of the first dice and Y the outcome of the second, with

	$$
	X(\Omega) = Y(\Omega) = \{1, \dots, 6\}
	$$

	and, $$
	p(x, y) = \P(X = x, Y = y) = \frac{1}{36}
	$$

	So here $p(x,y)$ is called the \emph{joint probability mass function} of the r.v. $X$ and $Y$.

	\vspace{1em}
	
	We can say $\P(X = x) = \sum_y \P(X = x \mid Y = y) = p_X(x)$ which is called the marginal over $X$ of $p(x, y)$

	Similarly, $\P(Y = y) = \sum_x \P(Y = y \mid X  = x) = p_Y(y$ which is the marginal over $Y$ of $p(x, y)$

	\vspace{1em}
	
	In this case of the dice we have,
	$$
	p_X(x) = p_Y(y) = \frac{1}{6} \quad \forall x,y
	$$

	\vspace{1em}
	
\end{eg}
\begin{eg}
	Consider the above example but now let $Z_+ = X + Y$ and $Z_- = X - Y$. So we have,$$
	Z_+ = \{2, \dots, 12\} \quad \text{and} \quad  Z_-= \{-5, \dots, 5\}
	$$


	Now we can say that,
	\begin{align*}
		Z_+  = 2 &\implies Z_- = 0\\
		Z_+  = 3 &\implies Z_- = \{1, -2\}\\
		Z_+  = 4 &\implies Z_- = \{-2, 0, 2\}\\
			 &\vdots
	\end{align*}


	We can ask $\P(Z_+ =  4 \text{ and } Z_- = -2) = \frac{1}{36}$  as there is only one possibility which is when $X = 1, Y = 3$.

	\vspace{1em}
	
	Notice that the set of possible values of $Z_-$ is like a rhomboid i.e. it increases linearly until 7 and then decreases linearly.

	\vspace{1em}
	
	We can ask $\P(Z_+ = 4) = \sum_z \P(Z_+ = 4 \text{ and } Z_i = z) = \frac{3}{36} = \frac{1}{12}$


	\vspace{1em}
	
\end{eg}

\begin{definition}
	If $x$ and $y$ are r.v. over $\Omega$   then,
	$$
	p(x, y) = \P(X = x \text{ and } Y = y)
	$$
	is called the joint p.m.f of $X $ and $Y$. And we must have,
	\begin{align*}
		p(x, y) \ge 0\\
		\sum_{x, y} p(x, y) = 1
	\end{align*}
\end{definition}

\section{Expected Values}

Given $X,Y$ we want $\E(X)$. We have,
\begin{align*}
	\E(X) &= \sum_x x p_X(x)\\
	      &= \sum_x x \sum_y \P(X = x \text{ and } Y = y)\\
	      &= \sum_{x,y} x \P(X = x \text{ and } Y = y)
\end{align*}

Similarly,
\begin{align*}
	\E(Y) &= \sum_{x,y} y \P(X = x \text{ and } Y = y)
\end{align*}

Given $(X,Y)$, I can think of $X$ as a function of $(X,Y)$. Take a less simple function, $$
Z = aX + bY \quad a, b \in \R
$$

We can ask $\E(Z)$ we have,
\begin{align*}
	\E(Z) &= \sum_z \P(Z =z )\\
	      &= \sum_{x, y} (ax + by) \P(X = x \text{ and } Y= y)\\
	      &= a \sum_{x,y} x \P(X = x \text{ and } Y = y) + b \sum_{x, y} y \P(X = x \text{ and } Y = y)\\
	      &= a \E(X) + b \E(Y)
\end{align*}

\begin{corollary}
	$$
	\E(aX + bY + c) = a\E(X) + b\E(Y) + c
	$$
\end{corollary}

Consider r.v $X, Y$ and $g: \R^2 \to \R$ so that, $$
	Z = g(X,Y)
$$


Since $X, Y$ are r.v we can say,
\begin{align*}
	\{\omega \mid X(\omega)  = x \text{ and } Y(\omega) = y\} = \{\omega \mid X(\omega) = x\}  \cap \{\omega \mid Y(\omega) = y\} \in \F
\end{align*}


We have,
\begin{align*}
	\E(Z) &= \E(g(X, Y))\\
	      &= \sum_{x,y} g(x,y) \P(X = x \text{ and } Y = y)
\end{align*}

\section{Covariance}

\begin{definition}
	Covariance is defined as,
	\begin{align*}
		Cov(X, Y) &= \E((X - \E(X)) (Y - \E(Y)))\\
			  &= \E(XY) - \E(X)\E(Y)
	\end{align*}
\end{definition}
\begin{note}
The covariance of $X$ with itself is just the variance of $X$.
\end{note}
\begin{note}
	We can used this to measure how two r.v are correlated.
\end{note}

We can define the correlatoin coefficient as,
$$
	\rho = \frac{Cov(X, Y)}{\sigma_X \sigma_Y}
$$
Where $\sigma_X^2 = Var(X)$ and $\sigma_Y^2 = Var(Y)$ and, $$
-1 \le \rho_{x, y} \le 1 $$
We also have,
\begin{align*}
	\rho_{X, Y} &= 1 \quad X = aY \quad a > 0\\
	\rho_{X, Y} &= -1 \quad X = bY \quad b < 0\\
	\rho_{X, Y} &= 0  \quad \text{ we say they are uncorrelated }
\end{align*}

\section{Independence}
\begin{definition}
	
If $X, Y$ are independent then if I know the value of $X$ it gives me no information on the value of $Y$. So we have,

\begin{align*}
	\P(Y = y \mid X = x) &= \P(Y= y)
\end{align*}
or that, 
\begin{align*}
	\P(Y = y \text{ and  } X = x) &= \P(X = x) \P(Y = y) \quad \forall x,y
\end{align*}


\end{definition}

\begin{theorem}
	If $X, Y$ are independent then we have $\E(XY) = \E(X) \E(Y)$
\end{theorem}
\begin{proof}
	We have, 
	\begin{align*}
		\E(XY) &= \sum_{x,y} xy \P(X = x \text{ and } Y = y)\\
		       &= \sum_{x,y} x y \P(X = x) \P(y = y)\\
		       &= \sum_{x,y} x   \P(X = x) \sum_y y \P(y = y)\\
			&= \E(X) \E(Y)
	\end{align*}

\end{proof}
\begin{remark}
	Which also means that we have $\Cov(XY) = 0$. However,
	$$
		Cov(X, Y) = 0 \not \implies \text{ independence }
	$$

	For instance consider we have possible values only $\{(0, 1), (1, 0), (-1, 0), (0, -1)\}$. So $X$ can take value $-1$ with probability $\frac{1}{4}$, $0$ with $\frac{1}{2}$ and $1$ with $\frac{1}{4}$. We have $\E(X) = 0, \E(Y) = 0$ and $XY = 0$. So we get $Cov(X,Y) = 0$. But if we know that value of $X$ then we know the possible value of $Y$.  More specifically we have,
	$$
		\P(X = 0) = \P(Y = 0)= \frac{1}{2}\quad \text{ but } \quad \P(X = 0 \text{ and } Y = 0) = 0
	$$
 
	\vspace{1em}
	
	Here $X,Y$ are not independent but $Cov(X,Y) = 0$. 
\end{remark}


\begin{remark}
	So if $X, Y$ are independent and $g,f$ are functions then we get,
	$$
		\E(g(X) f(Y)) = \E(g(X)) \E(f(Y))
	$$
\end{remark}	

\begin{definition}
In general given $X_{1}, \dots, X_n$ then the joint p.m.f is, 
\begin{align*}
	\P(X_{1} = x_{1} \dots X_n = x_n)
\end{align*}
\end{definition}

\begin{remark}
	Independence means that $\P(X_{1} = x_{1} \text{ \& } \dots \text{ \& } X_n = x_n)  = \prod_{i = 1}^{n} \P(X_i = x_i)$

\vspace{1em}

We can further show this is true for any subset of $X$ by summing over $X_i$ to exclude $X_i$.

\vspace{1em}

We can recognize independence if, $p(x,y) = f(x)g(y)$. For instnace,
\begin{align*}
	p(x, y) &= \frac{e^{-\lambda - \mu}}{x! y!} \mu^{x} \lambda^{y}\\
		&= \left ( \frac{e^{-\lambda - \mu}}{x!} \mu^{x} \right ) \left ( \frac{\lambda^{y}}{y!} \right )
\end{align*}

So we're able to write $p(x,y)$ as the product of two functions dependent on $x,y$.

\end{remark}

