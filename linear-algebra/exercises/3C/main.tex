\documentclass[a4paper]{report}
\input{../../preamble.tex}
\title{Linear Alebgra 3C}
\author{Aamod Varma}
\begin{document}
\maketitle
\date{}

\section*{3C}
\subsection*{Problem 1}
\begin{proof}
    Let us assume the contrary that the matrix of $T$ can be less than $\dim \range T$ non-zero entries. Now consider a basis of $W$ as $w_1,\dots,w_n$ such that $w_1,\dots,w_k$ spans $\range T$. Now because we have only $\dim \range T - 1$ non-zero entries in our matrix we have $r < k$ non-zero entries.

    So we would have a maximum of $r$ columns with non-zero entries in our matrix. By definition now, 
    \begin{align*}
        Tv_k = A_{1k}w_1 + \dots + A_{nk}w_n\\
    \end{align*}
    We also know that the definition of linear maps maps a vector in the basis of V to a vector in W. So,
    $$ Tv_1 = w_1 ,\dots,Tv_m = w_m$$ 

    However because we have a maximum of $r$ columns that are non-zero, we can only map  any v to a linear combination of $w_1,\dots,w_r$. But this means that $\dim range = r$ as any $v \in V$ is mapped to $r$ linearly independent vectors. But this contradicts our assumption that $\dim range = k$. Hence our assumption must be wrong.
\end{proof}



\subsection*{Problem 2}
\begin{proof}
    Consider $\range T$ is spanned by $w_1$. Now let us extend this basis to $w_1,\dots,w_n$. Let us rewrite this basis as, 
    $$ w_1 - w_2,w_2 - w_3,\dots,w_{n-1} - w_n,w_{n} $$ 

    Let us show this is a basis first, 
    \begin{align*} 
        a_1(w_1-w_2) + \dots + a_{n-1}(w_{n-1} - w_n) + a_n(w_n) &= 0\\
        a_1w_1 + (a_2-a_1)w_2 + \dots + (a_n - a_{n-1})w_n&= 0
    \end{align*}
    We nkow that $w_1,\dots,w_n$ is lin independent. Hence $a_1 = a_2 - a_1 = \dots = 0$.
    If $a_1 = 0$ the $a_2 - 0 = 0$and so on which means $a_1 = \dots = a_n = 0$.

    Hence the sum of the vectors in our basis all equal to $w_1$. Let this basis be as follows, 
    $$ w_1' = w_1 - w_2, \dots,w_n' = w_n $$ 

    Now we knwo that our range is $1$ which means  $\exists v \in V$ such that $Tv_1 = w_1$. Let us extend this basis to $v_1,\dots,v_n$. Now let us modify this basis as follows, 
    $$ v_1, v_2 + v_1,\dots, v_n + v_1 $$ 

    First we show this is a basis, 
    \begin{align*}
        a_1v_1 + a_2(v_2 + v_1) + \dots + a_n(v_n + v_1) &= 0 \\
        v_1(a_1 + \dots + a_n) + a_2v_2 + \dots + a_nv_n &= 0\\
    \end{align*}
    $v_1,\dots,v_n$ is linearly independent hence, $a_2,\dots,a_n = 0$ and $a_1+ \dots + a_n = 0$ so $a_1 + 0 = 0$ and $a_1 = 0$. Hence our new basis is constructed as $v_1' = v_1, v_k' = v_k + v_1$.

    We nkow that  
    \begin{align*}
    Tv_1' &= w_1\\
    Tv_k' &= T(v_k) + T(v_1') \text{ for $1 < k \le n$}
    \end{align*}
    Now because $\range T = 1$ either $T(v_k) \in \range T \implies T(v_k) = \lambda_k w_1$ or $T(v_k) = 0 \implies T(v_k')= T(v_1') = w_1$. If $T(v_k) \in \range T$. Let us rewrite $v_k'$ as $v_k' = \frac{v_k+ v}{\lambda_k + 1}$.


    Essentially we constructed a basis of $V$ such that $v_1',\dots,v_n'$ such that for $Tv_k'$ it is equal to $w_1$ or in other words, 
    \begin{align*}
        Tv_1' &= w_1\\
              &\dots\\
        Tv_n' &= w_1
    \end{align*}

    But we rewrite $w_1$ as sum of basis of W so we have, 
    \begin{align*}
        Tv_1' &= w_1' + \dots + w_n'\\
              &\dots\\
        Tv_n' &= w_1' + \dots + w_n'
    \end{align*}

    Now by how matrices are defined we have constructed a basis of V and $W$ such that all entires of our matrix is just 1s.



\end{proof}
% \begin{proof}
%     $\impliedby$ 

%     First we know that if $\dim \range T = 1$ then  $\range T$ is spanned by a single arbitrary non-zero vector $w$. We nkow as $T$ is a linear map we can construct a basis $V$, $v_1,\dots,v_n$ as follows,


%     First choose any $v_1 \in V$ such that $Tv_1 = b_1w$. Now let us extend this basis to $V$ as $v_1,\dots,v_m$. We change this basis to $v_1,v_2+v_1,\dots,v_m+v_1$. It is easy to show that this list is linearly independent and hence is a basis.
    
%     For any vector in this basis we know that $T(v_k) = b_kw$ such taht $b_k \ne 0$ as $T(v_k) = T(v_{k'}) + T(v) = \lambda + b_1w$ where $\lambda$ may or may not be zero.
%     where any $b \ne 0$ because for any $v$ such that $P(v) = 0$

%     Hence we find, 
%     $$ T(v_1) = b_1w,\dots,T(v_n) = b_nw $$

%     Now we divide each side by $\frac{1}{b_k}$ to get, 
%     $$ T(\frac{v_1}{b_1})= w,\dots,T(\frac{v_n}{b_n}) = w $$ 

%     We can construct a basis of $W$ from $w$ to $w,\dots,w_n$. Hence we have our matrix as,  
%     $$ T(v_1') = 1w + \dots + 0w_n $$ 
%     $$ \dots$$
%     $$ T(v_n') = 1w + \dots + 0w_n $$ 

%     Now we know that $M(T)$ can be constructed with all 1s.
% \end{proof}

\subsection*{Problem 3}
\begin{proof}
    (a). We know that the matrix of T is defined as, 
    $$ T(v_k) = A_{1k}w_1 + \dots + A_{nk}w_n $$ 
    and that of $S$ is defined as, 
    $$ S(v_k) =B_{1k}w_1 + \dots + B_{nk}w_n $$

    And for $(S+T)$ let it be, 
    $$ (S+T)v_k = C_{1k}w_1 + \dots + C_{nk}w_n $$ 
    We know that $(S+T)v = Sv + Tv$. So for the basis of $V$ given we have, 
    \begin{align*}
        (S+T)v_k &= Sv_k + Tv_k\\
                 &= (A_{1k}+B_{1k})w_1 + \dots + (A_{nk} + B_{nk})w_n
    \end{align*}

    So we have $C_{j,k} = A_{jk} + B_{jk}$. Which essentially means $M(S+T) = M(S) + M(T)$


    \vspace{2em}
    (b). We have $M(T)$ defined as  follows,  
    \begin{align*}
     (T) v_k  &= A_{11}w_n + \dots + A_{nk}w_n \\
     \lambda \times  (T)v_k &=  (\lambda A_{11}) w_1 + \dots + (\lambda A_{nk} w_n)\\
    \end{align*}
    Or in other words the matrix $\lambda M(T)$ is  defined as, 
    \begin{align*}
     \lambda \times  (T)v_k &=  (\lambda A_{11}) w_1 + \dots + (\lambda A_{nk} w_n)\\
    \end{align*}

    Now consider the matrix of $\lambda T$ we have, 
    \begin{align*}
     (T) v_k  &= A_{11}w_n + \dots + A_{nk}w_n \\
     (\lambda T)v_k &= (\lambda) \times  Tv_k\\
                    &= (\lambda A_{11}) w_1 + \dots + (\lambda A_{nk} w_n)\\
    \end{align*}

    Hence as each element of $\lambda M(T)$ is the same as that of $M(\lambda T)$ we have $M(\lambda T) = \lambda M(T)$


\end{proof}

\subsection*{Problem 4}
\begin{proof}
    Consider a basis of $P_3$ as $p_1,\dots,p_4$ and a basis of $P_2$ as $1 + x + x^2$. By definitino of linear map we have, 

    \begin{align*}
        Dp_1 &= 1 \\
        Dp_2 &= x\\
        Dp_3 &= x^2\\
        Dp_4 &= 0 \\
    \end{align*}

    So we have $p_1 = x, p_2 = x^2, p_3 = x^{3}, p_4 = 1$ as a basis of $P_3$


\end{proof}
\subsection*{Problem 5}
\begin{proof}
    Let $\dim \range T = r$ and consdier a basis for $\range T$ as $w_1,\dots,w_r$. Let us nwo extedn this basis to $w_1,\dots,w_m$. Now as $w_1,\dots,w_r \in \range T$  we nkow $\exists v_1,\dots,v_r$ such that $Tv_1 = w_1,\dots,Tv_r = w_r$. It is easy to show that $v_1,\dots,v_r$ is linearly independent.

    Now extend our $v_1,\dots,v_r$ to a basis of $V$ as $v_1,\dots,v_n$. So we have, 
    \begin{align*}
        Tv_1 &= w_1\\
             &\dots\\
        Tv_r &= w_r\\
             &\dots \\
        Tv_n &= 0\\
    \end{align*}

    Based on how we define our matrix we can write, 
    \begin{align*}
        Tv_1 &= 1w_1 + \dots + 0w_r + \dots + 0w_m\\
        & \dots\\
        Tv_r &= 0w_1 + \dots + 1w_r + \dots + 0w_m\\
        & \dots\\
        Tv_n &= 0w_1 + \dots + 0w_r + \dots + 0w_m\\
    \end{align*}

    Hence for only row k column k we have all ones and rest are all zeores.

\end{proof}


\subsection*{Problem 6}
\begin{proof}
    First consider if $Tv_1 = 0$. In this case we have $Tv_1 = 0w_1 + \dots + 0w_n$ where $w_1,\dots,w_n$ is a basis of $W$. So we have all zeroes in the first column.

    Now consider  if $Tv_1 = w_1 \ne 0$. Let us now extend this basis to $w_1,\dots,w_n$. And we have $Tv_1 = 1w_1 + \dots + 0w_n$. So we have all 0s except for a 1 in the first column first row.
\end{proof}
\subsection*{Problem 8}
\begin{proof}
    We need to show $(AB)_{j,.} = A_{j,.}B$

    First we have, 
         $$AB_{j,k} &= \sum_{r=1}^{n} A_{j,r}B_{r,k}\$$

         So, 
         \begin{align*}
             AB_{j,.}  &= (\sum_{r=1}^{r=n} A_{j,r}B_{r,1},  \dots, \sum_{r=1}^{r=n} A_{j,r}B_{r,n})\\
                       &= (A_{j,1},\dots,A_{j,n}) B\\
                       &= (A_{j,.}B_{.,1}, \dots , A_{j,.}B_{.,n})\\
                       &= A_{j,.}B
         \end{align*}

\end{proof}

\subsection*{Problem 9}
\begin{proof}
    We have, 
    \begin{align*}
        (aB)_{1,k} &= \sum_{r=1}^{n} A_{1,n}B_{n,k}\\
        (aB) &= (\sum_{r=1}^{n} A_{1,n}B_{n,1},\dots,\sum_{r=1}^{n} A_{1,n}B_{n,1})\\
    \end{align*}

    Now we have, 
    $$ a_1B_{1,.} + \dots + a_nB_{n,.} = a_1B_{11}+ \dots + a_n B_{n,1} , \dots, a_1B_{n1} + \dots + a_n B_{nn}$$ 
    $$ = (\sum_{r=1}^{n} a_{n}B_{n,1},\dots,\sum_{r=1}^{n} a_{n}B_{n,1}) $$ 

    Hence we have our equality.
\end{proof}
\subsection*{Problem 10}
\begin{proof}
    Let $A = (1,0;0,0)$ and $B = (0,1;0,0)$. We have AB = (0,1;0,0) and $BA = (0,0;0,0)$
\end{proof}

\subsection*{Problem 11}
\begin{proof}
    Let $B + C = X$ such that,
    First be have $(B + C)_{j,k} = B_{j,k} + C_{j,k} = X_{j,k}$

    So we have, 
    $$ A(B + C) = AX $$ 
    \begin{align*}
        (AX)_{j,k} &= \sum_{r=1}^{n} A_{j,r} X_{r,k}\\
                   &= \sum_{r=1}^{n} A_{j,r}(B_{r,k} + C_{r,k})\\
                   &= \sum_{r=1}^{n} A_{j,r}(B_{r,k})  + A_{j,r}(C_{r,k})\\
                   &= \sum_{r=1}^{n} A_{j,r}(B_{r,k})  + \sum_{r=1}^{n} A_{j,r}(C_{r,k})\\
                   &= AB_{j,k} + AC_{j,k}
    \end{align*}

    Hence we have $A(B + C) = AB + AC$
\end{proof}

\subsection*{Problem 12}
\begin{proof}
    We know if $T$ and $S$ are linear map from $U,V$ and $V,W$ respectively then $M(ST) = M(S)M(T)$

    Let  $A = M(T), B = M(S), C = M(R)$ for a linear map $T,S,R$ So we have, 
    \begin{align*}
        (AB)C &= (M(T)M(S))M(R)\\
              &= (M(TS))M(R)\\
              &= M((TS)R)\\
              &= M(T(SR))\\
              &= M(T)M(SR)\\
              &= M(T)(M(S)M(R))\\
              &= A(BC)
    \end{align*}

\end{proof}

\subsection*{Problem 13}

\begin{proof}
    
We know that 
$$ (AA)_{j,k} = \sum_{r = 1}^n A_{j,r} A_{r,k} $$ 

$$ (A(AA))_{j,k} = \sum_{r = 1}^n A_{j,r} (AA)_{r,k}$$ 

$$ (A^{3})_{j,k} = \sum_{p = 1}^n A_{j,r}( \sum_{x = 1}^n A_{r,x} A_{x,k}  )$$ 

$$ (A^{3})_{j,k} = \sum_{p = 1}^n \sum_{x = 1}^n A_{j,r} A_{r,x} A_{x,k}$$ 


\end{proof}
\subsection*{Problem 14}
\begin{proof}
    To show that the functino is a linear map we need to show additvity and homogenity. Consider $A \in F^{m,n}$  and $B \in F^{m,n}$. 

    1. Additive. 

   We need to show that $(A+B)^{t} = A^{t} + B^{t}$ . First we know that $(A+B)_{j,k} &= A_{j,k} + B_{j,k}$

   \begin{align*}
       (A+B)^{t}_{j,k} &= (A+B)_{k,j}\\
                   &= A_{k,j} + B_{k,j}\\
                   &= A^{t}_{j,k} + B^{t}_{j,k}
   \end{align*}

   So we have $(A+B)^{t} = A^{t} + B^{t}$

    2. Scalar multiplication.

    We need to show that $(kA)^{t} = k A^{t}$

    We have $(kA)_{j,k} = k(A_{j,k})$ . So $$(kA)^{t}_{j,k} = (kA)_{k,j}$$
    $$ = k A_{k,j} $$ 

    So we have $(kA)^{t} = k A^{t}$


\end{proof}

\subsection*{Problem 15}
\begin{proof}
    We have, 
    $$ AC_{j,k} =  \sum_{r=1}^{n} A_{j,n} C_{n, k}$$

    So $AC^{t}_{j,k} = AC_{k,j} $ which is, 
    $$ = \sum_{r=1}^{n} A_{k,r} C_{r, j}$$  

    Now $C^{t}_{j,k} = C_{k,j} $ and $A^{t}_{j,k} = A_{k,j}$ . So $$(C^{t}A^{t})_{j,k} = \sum_{r=1}^{n} C_{r,j} A_{k,r}$$
    
    $$  = \sum_{r=1}^{n} A_{k,r} C_{r, j} $$ 
    $$ = (AC)^{t}_{j,k} $$ 


    

\end{proof}

\subsection*{Problem 16}
\begin{proof}
    $\impliedby$
    First we know that for any rank $k$ matrix we can write it as a prodcut of $RC$ such that $R = m \times  k$ and $C =k \times  n$ if $A$ is a $m \times n$ matrix. So if $A$ is a rnak 1 matrix we can write it as a $m \times  1$ times $1 \times n $ product of matrices.

    Let $C = (c_1,\dots,c_m)^{T}$ and $R = (d_1,\dots,d_n)$

    So we have $A_{j,k} = C_{j,1}R_{1,k} = c_jd_k$

    $\implies$
    We have $A_{j,k} = c_jd_k$. Let  $C \in F^{m,1}$ where $C_{j,1} = c_j$. Now we have  $A_{.,k} = d_k C$. So each column of  $A$ is a scalar multiplcatino of $C$. Which means that our matrix $A$ has rank 1.
\end{proof}



\subsection*{Problem 17}
\begin{proof}
    (a) $\implies$ (b)
    We need to show that $A_{.,1},\dots,A_{.,n}$ are linearly independent or that, if  
    $$ \lambda_1A_{.,1}  + \dots + \lambda_n A_{.,n} = 0$$ The only solution is all  $\lambda= 0$

    By definitino of matrix we have,  
    $$ Tv_k= A_{1,k}u_1 + \dots + A_{n,k}u_n $$ 

    Consider $\lambda_k Tv_k = \lambda_k A_{1,k}u_1 + \dots + \lambda_k A_{n,k}$ 

    Now as $T$ is injective we have $\lambda_k T v_k = 0$. This can only be true if $\lambda_k = 0$. Which means our columns are linearly independent.
\end{proof} 


\end{document}
